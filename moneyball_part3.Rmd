---
title: "Moneyball Part 3"
author: "datadavidz"
date: "5/11/2021"
output: html_document
---

My own version of TidyX Episode 55 where we fit decision tree and random forest models to the pitch f/x data.
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(tidymodels)
library(here)
library(lubridate)

library(rpart.plot)
library(vip)

here::i_am("moneyball_part3.Rmd")
```

Load data, minor preprocessing, and do the train/test split using rsample methodology.
Calculation of the split proportion by date/time could be simplified to just taking 2/3 but wanted to be consistent with TidyX.
```{r}
#training
set1 <- readRDS(file = here("data", "2016_04_21_to_2016_04_23_pitch.rds"))
set2 <- readRDS(file = here("data", "2016_04_24_to_2016_04_26_pitch.rds"))
#testing
set3 <- readRDS(file = here("data", "2016_04_27_to_2016_04_30_pitch.rds"))

#Keeping date and time
pitch_raw <- set1 %>%
  bind_rows(set2) %>%
  bind_rows(set3) %>%
  mutate(pitch_date = str_replace(y, "T", " "),
         pitch_date = str_remove(pitch_date, "\\..*$"),
         pitch_date = ymd_hms(pitch_date))

#Need to know the max date for set 2 for splitting purpose
set2_max_date <- set2 %>%
  mutate(pitch_date = str_replace(y, "T", " "),
         pitch_date = str_remove(pitch_date, "\\..*$"),
         pitch_date = ymd_hms(pitch_date)) %>%
  summarize(max_date = max(pitch_date, na.rm = TRUE)) %>%
  pluck(., 1)

pitch_clean <- pitch_raw %>%
  filter(!pitch_type %in% c("KN", "IN", "PO", "EP", "FO", "SC"),
         !is.na(pitch_type)) %>%
  select(pitch_date, pitch_type, start_speed, end_speed, pfx_x, pfx_z, px, pz, x0, z0, vx0, vz0) %>%
  drop_na()

prop <- nrow(pitch_clean %>% filter(pitch_date <= set2_max_date )) / nrow(pitch_clean)
```

Create the initial split based on time (like done for TidyX)
```{r}
pitch_split <- initial_time_split(data = pitch_clean, prop = prop)
pitch_train <- training(pitch_split)
pitch_test <- testing(pitch_split)
```

Create the recipe
```{r}
rpart_rec <- recipe(pitch_type ~ ., data = pitch_train) %>%
  update_role(pitch_date, new_role = "id") %>%
  prep()

rpart_rec
```
Specify the decision tree model
```{r}
rpart_spec <- decision_tree() %>%
   set_engine("rpart") %>%
   set_mode("classification")

rpart_spec
```

Create the workflow
```{r}
rpart_wf <- workflow() %>%
  add_recipe(rpart_rec) %>%
  add_model(rpart_spec)
```

Fit the decision tree
```{r}
rpart_res <- fit(rpart_wf, data = pitch_train)

rpart_res
```

Plot the decision tree
```{r}
rpart.plot(rpart_res$fit$fit$fit, roundint = FALSE) #yep - not ideal
```

Do the fit and the predictions based on the initial data split.
```{r}
rpart_fit <- rpart_wf %>% last_fit(pitch_split, metrics = metric_set(accuracy, kap))

rpart_fit %>% collect_metrics()
```
Show the confusion matrix for pitch predictions
```{r}
rpart_fit %>% collect_predictions() %>% conf_mat(truth = pitch_type, estimate = .pred_class)
```

### Random Forest Model

Same data and split as above

Create the recipe
```{r}
rf_rec <- recipe(pitch_type ~ ., data = pitch_train) %>%
  update_role(pitch_date, new_role = "id") %>%
  prep()

rf_rec
```

Specify the decision tree model
```{r}
rf_spec <- rand_forest(mtry = 6) %>%
   set_engine("randomForest") %>%
   set_mode("classification")

rf_spec
```

Create the workflow
```{r}
rf_wf <- workflow() %>%
  add_recipe(rf_rec) %>%
  add_model(rf_spec)
```

Fit the decision tree
```{r}
set.seed(567)

rf_res <- fit(rf_wf, data = pitch_train)

rf_res
```
Variable importance plot
```{r}
vip(rf_res$fit$fit$fit, geom = "point", aesthetics = list(color = "steelblue")) +
  labs(title = "Pitch F/X Variable Importance for Random Forest Model")
```

```{r}
rf_fit <- rf_wf %>% last_fit(pitch_split, metrics = metric_set(accuracy, kap))

rf_fit %>% collect_metrics()
```

```{r}
rf_fit %>% collect_predictions() %>% conf_mat(truth = pitch_type, estimate = .pred_class) %>% summary()
```

Within class accuracy
```{r}
rf_fit %>% 
  collect_predictions() %>% 
  select(pitch_type, predicted = .pred_class) %>%
  count(pitch_type, predicted) %>%
  group_by(pitch_type) %>%
  summarize(N = sum(n),
            matches = sum(ifelse(pitch_type == predicted, n, 0)),
            within_acc = matches / N, 
            .groups = "drop")
```

### Tune the Random Forest Model

Never do in this order but we'll go back and tune a random forest model. Data, split, recipe is all the same.

Create cross-validation folds for tuning
```{r}
set.seed(234)

pitch_folds <- vfold_cv(pitch_train, v = 5)
pitch_folds
```

Create the model spec for tuning
```{r}
rf_tune_spec <- rand_forest(mtry = tune()) %>%
   set_engine("randomForest") %>%
   set_mode("classification")

rf_tune_spec
```

Create the tune grid
```{r}
rf_grid <- grid_regular(mtry(c(3L, 7L)), levels = 5)

rf_grid
```

Create the workflow
```{r}
rf_tune_wf <- workflow() %>%
  add_recipe(rf_rec) %>%
  add_model(rf_tune_spec)
```

Tune the random forest using folds
```{r}
doParallel::registerDoParallel()

rf_tune <- tune_grid(
  rf_tune_wf,
  resamples = pitch_folds,
  grid = rf_grid,
  metrics = metric_set(accuracy, kap)
)
```

Analyze tune results
```{r}
rf_tune %>% show_best("kap", n = 5)
rf_tune %>% show_best("accuracy", n = 5)

rf_tune %>% autoplot()
```

Finalize neighbors parameter at 5.  This could also be done by select_best but to be consistent with TidyX we use 5.
```{r}
rf_final_wf <- rf_tune_wf %>% finalize_workflow(tibble(mtry = 5))
```

Final fit with split and mtry = 5
```{r}
rf_final_fit <- rf_final_wf %>% last_fit(pitch_split, metrics = metric_set(accuracy, kap))

rf_final_fit %>% collect_metrics()
```

More detailed summary statistics
```{r}
rf_final_fit %>% collect_predictions() %>% conf_mat(truth = pitch_type, estimate = .pred_class) %>% summary()
```

Within class accuracy
```{r}
rf_final_fit %>% 
  collect_predictions() %>% 
  select(pitch_type, predicted = .pred_class) %>%
  count(pitch_type, predicted) %>%
  group_by(pitch_type) %>%
  summarize(N = sum(n),
            matches = sum(ifelse(pitch_type == predicted, n, 0)),
            within_acc = matches / N, 
            .groups = "drop")
```





